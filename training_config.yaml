# Bespoke Punk SDXL Training Configuration
# Edit this file to customize your training settings

# Dataset paths
images_dir: "./FORTRAINING6/all"
captions_dir: "./FORTRAINING6/oldtext"
output_dir: "./models/bespoke_punk_sdxl"

# Base model
base_model: "stabilityai/stable-diffusion-xl-base-1.0"
# Alternative: "stabilityai/stable-diffusion-xl-base-1.0"

# Training settings
resolution: 512  # 512, 768, or 1024
train_batch_size: 4  # Adjust based on your GPU (1-8)
num_train_epochs: 120  # More epochs = more training
gradient_accumulation_steps: 1  # Increase if batch_size=1

# Learning settings
learning_rate: 0.00008  # 8e-5, conservative for stable training
lr_scheduler: "cosine"  # cosine, linear, constant
mixed_precision: "fp16"  # fp16, bf16, or no

# LoRA settings
lora_rank: 16  # 8, 16, 32 (higher = better quality, larger file)
lora_alpha: 16  # Usually same as lora_rank
lora_dropout: 0.0  # 0.0 to 0.1

# Checkpoint settings
save_steps: 500  # Save checkpoint every N steps
validation_steps: 100  # Log validation every N steps

# Monitoring (optional)
wandb_project: "bespoke-punk-sdxl"
wandb_api_key: "495752e0ee6cde7b8d27088c713f941780d902a1"  # Your W&B key

# Other settings
seed: 42
num_workers: 2

# GPU-specific presets (uncomment one)

# --- RTX 4090 / A6000 (24-48GB VRAM) ---
# train_batch_size: 4
# gradient_accumulation_steps: 1
# resolution: 512

# --- RTX 3090 (24GB VRAM) ---
# train_batch_size: 2
# gradient_accumulation_steps: 2
# resolution: 512

# --- RTX 3080 (10-12GB VRAM) ---
# train_batch_size: 1
# gradient_accumulation_steps: 4
# resolution: 512

# --- High Memory Setup (A100, A6000) ---
# train_batch_size: 8
# gradient_accumulation_steps: 1
# resolution: 768

# Training duration estimates (193 images):
# - 100 epochs ≈ 19,300 steps ≈ 4-6 hours on RTX 4090
# - 120 epochs ≈ 23,160 steps ≈ 5-7 hours on RTX 4090
# - 150 epochs ≈ 28,950 steps ≈ 6-9 hours on RTX 4090
